{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speed_up_pytorch_inference_with_onnx.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ClrAqwup-mpl",
        "yN2SeIRl-kG4",
        "Gsikc2hJC-Al",
        "HHLKtKGtFvTm",
        "wWMpSPHzGhoF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi9716/Tutorials/blob/master/speed_up_pytorch_inference_with_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiihKUB6CRLy"
      },
      "source": [
        "## what is onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aevGZacCC1U2"
      },
      "source": [
        "\n",
        "\n",
        "> The Open Neural Network Exchange (ONNX) is an open-source artificial intelligence ecosystem that allows us to exchange deep learning models. This help us to make model portable.\n",
        "\n",
        "> At the high level onnx allow us to move our model in diffrent deep learning framework currently there is native support in ONNX for PyTorch, CNTK, MXNet, and Caffe2 but there are also converters for TensorFlow and CoreML. Also ONNX makes it easier to access hardware optimizations.\n",
        "\n",
        "\n",
        "\n",
        "> For this blog we will look how to convert pytorch model into onnx format and inference into cpu systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE8X2IgBKXkq"
      },
      "source": [
        "![alt text](https://microsoft.github.io/ai-at-edge/assets/images/ONNX.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOHiGq-aJr8s"
      },
      "source": [
        "## Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBkXdFKFKaFo"
      },
      "source": [
        "Following is list of providers you can use as per your hardware resources. We will use CPUExecutionProvider for this session.\n",
        "```\n",
        "providers = [\n",
        "  \"CUDAExecutionProvider\",\n",
        "  \"CPUExecutionProvider\",            \n",
        "  \"TensorrtExecutionProvider\",\n",
        "  \"DnnlExecutionProvider\",          \n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuN428rrKtWl"
      },
      "source": [
        "**Keep in mind**\n",
        "\n",
        "*   When you pass input to onnx you have to make dictionary of inputs with same name as you provide at time of export.\n",
        "*   Onnx takes numpy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClrAqwup-mpl"
      },
      "source": [
        "## Train Mnist model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7p9jGEO18Yo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ad3d903-8187-447c-b8d5-061f24bbf705"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=True,\n",
        "                        help='For Saving the current Model')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'batch_size': args.batch_size}\n",
        "    if use_cuda:\n",
        "        kwargs.update({'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True},\n",
        "                     )\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    sys.argv = \" \"\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333409\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.268057\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.831820\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.586374\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.334603\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.390572\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.274378\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.226873\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.429131\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.224970\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.330837\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.265526\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.398376\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.251901\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.334751\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.072920\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.095864\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.199344\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.260538\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.090786\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.289219\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.110628\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.142823\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.076382\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.120273\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.114172\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.115733\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.145087\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.112840\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.063313\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.239670\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.179774\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.234174\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.270639\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.215247\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.029287\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.067572\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.154349\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.223753\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.083312\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.167660\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.050531\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.246149\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.078778\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.202517\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.133554\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.092537\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.068628\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.269806\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.031815\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.152192\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.039337\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.050666\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.168080\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.190800\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.050215\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.100481\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.086205\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.236123\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.028725\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.140623\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.139182\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.183783\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.143267\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.126526\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.224059\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.127473\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.132385\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.083168\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.171894\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.063048\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.086888\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.055762\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.076823\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.020112\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.060534\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.057230\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.064691\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.149541\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.085013\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.131799\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.086973\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.081079\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.153690\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.279272\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.058334\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.169417\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.065850\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.029773\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.075187\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.110722\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.034830\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.057845\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.131163\n",
            "\n",
            "Test set: Average loss: 0.0596, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.068899\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.056537\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.058741\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.040915\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.021887\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.100058\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.079072\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.005850\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.036023\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.035751\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.036036\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.044886\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.058691\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.025300\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.113433\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.204955\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.051641\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.046728\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.172520\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.088891\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.042351\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.129413\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.096572\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.033292\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.072325\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.149748\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.009002\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.018937\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.058164\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.178914\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.020271\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.160964\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.210953\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.076401\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.051454\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.017733\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.104035\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.086012\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.013325\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.059589\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.026937\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.156025\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.182993\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.096227\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.100992\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.073890\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.155587\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.145905\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.043785\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.008680\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.083177\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.167347\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.063251\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.066650\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.113529\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.144670\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.079105\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.125355\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.056801\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.371919\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.064123\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.079231\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.068310\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.184773\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.095613\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.257289\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.062563\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.241643\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.005959\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.052639\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.098812\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.029425\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.158476\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.033192\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.047408\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.014390\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.036237\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.027538\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.038776\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.003348\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.168367\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.085178\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.024909\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.055695\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.005674\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.003737\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.010234\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.030413\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.023026\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.102061\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.220897\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.142607\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.044522\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.026976\n",
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.016178\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.082552\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.046888\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.161367\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.010203\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.017927\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.095753\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.044638\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.047912\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.056106\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.009779\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.011497\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.011317\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.035523\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.027104\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.039831\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.154989\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.146077\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.027179\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.009916\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.068082\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.111214\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.102087\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.039985\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.078496\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.051562\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.145195\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.039120\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.134531\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.070562\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.026562\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.032186\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.089722\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.036280\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.007524\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.067127\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.027015\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.009414\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.046221\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.191100\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.024233\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.006465\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.136196\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.009535\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.026436\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.101499\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.093237\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.025941\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.033275\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.004561\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.061429\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.032476\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.007113\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.197545\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.009752\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.009027\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.002568\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.006125\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.016916\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.038204\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.009829\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.003198\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.011071\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.008499\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.072256\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.039800\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.012719\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.039022\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.016023\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.076358\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.016741\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.016327\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.127664\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.027024\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.058161\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.083988\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.033791\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.040401\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.097435\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.052402\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.089529\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.140996\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.033893\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.014014\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.004597\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.034825\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.034864\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.041560\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.085684\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.079144\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.048286\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.037545\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.114586\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.021913\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006381\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.035306\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.012546\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.008692\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.104183\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.097726\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.024826\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.003125\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.045885\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.123276\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.338022\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.010491\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.153792\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.023585\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.005532\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.014645\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.074919\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.018404\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.006814\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.063565\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.160042\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.027170\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.028643\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.015766\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.044292\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.017181\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.053456\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.133530\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.018004\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.001480\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.046620\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.056140\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.036829\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.023371\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.036886\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.036568\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.018465\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.010979\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.029137\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.053288\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.032147\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.004235\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.041847\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.006371\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.004639\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.026418\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.007566\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.010353\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.056364\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.019979\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.020133\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.018552\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.018903\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.004119\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.118952\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.125124\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.063759\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.004725\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.004043\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.058793\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.004529\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.164654\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.020854\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.029674\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001758\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.099645\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.017835\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.014594\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.075936\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.039327\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.044175\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.032834\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.015510\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.058354\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.116945\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.060285\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.032964\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.024637\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.007307\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.003588\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.066671\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.188306\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.049531\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.057671\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.075927\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.022922\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.011141\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.003954\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.030566\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.024548\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.002693\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.003703\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.012849\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.234886\n",
            "\n",
            "Test set: Average loss: 0.0317, Accuracy: 9896/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.063873\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.006020\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.092178\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.084384\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.040512\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.011151\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.006772\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.017306\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.185878\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.006105\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.092709\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.030748\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.036863\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.018165\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.013369\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.019437\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.101072\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.012802\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.033243\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.004315\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070459\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.014613\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.003170\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.028231\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.033972\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.013012\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.021474\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.059091\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.038080\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.029183\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.013249\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.052748\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.015123\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.000782\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.044017\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.042066\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.002084\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.019377\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.005095\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.033081\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.016470\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.036909\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.005375\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.028296\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.007074\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.004378\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.054785\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.121631\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.032712\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.025687\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.007796\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.077907\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.092062\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.009291\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.080602\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.059663\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.002971\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.019612\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.007485\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.005908\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.035482\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.005661\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.023137\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.004412\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.014227\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.074459\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.013375\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.003041\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.108063\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.004947\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000696\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.049680\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.088999\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.006557\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.031444\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.005479\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.042340\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.002017\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.012684\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.037930\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.009129\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.053702\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.020376\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.026886\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.005311\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.015415\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.005098\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.121105\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.008212\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.023781\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.011896\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.089698\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.037372\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.013450\n",
            "\n",
            "Test set: Average loss: 0.0290, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.055064\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.016290\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.009178\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.009835\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.019823\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.112318\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.052706\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.094717\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.007055\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.001623\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.012622\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.064775\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.020420\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.014015\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.017709\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.002024\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.006385\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.097586\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.090577\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.040928\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.015920\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.019171\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.013563\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.004200\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.015355\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.080532\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.003831\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.033194\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.029530\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.111289\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.157338\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.011044\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.003696\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.012174\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.032997\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.021096\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.004143\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.063386\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.036379\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.031136\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.002347\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.012797\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.017160\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.018350\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.015058\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.003634\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.010068\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.075891\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.005210\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.047527\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.092508\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.029495\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.046522\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.023160\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.004974\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.002336\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.001557\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.004447\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.009058\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.104078\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.062229\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.081196\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.013003\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.057333\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001180\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.081341\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.067011\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.312311\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.076701\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.025934\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.039477\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.006568\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.050759\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.014385\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.040722\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.004742\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.025504\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.024907\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.029507\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.026067\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.038059\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.005319\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.029250\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.144496\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.041812\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.020524\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.008086\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.014723\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.051804\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.028950\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001797\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.005626\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.040525\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.007157\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.039587\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.030922\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.004256\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.010972\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.008409\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.012705\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.027975\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.010681\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.011564\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.190048\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.009265\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.015650\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.012605\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.030416\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001341\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.018349\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.021549\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.051222\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.018052\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.002182\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.038152\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.001821\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.011362\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.001301\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.002996\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.020138\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.056493\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.006185\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.008360\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.047065\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.004963\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.092008\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.011296\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.079909\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.022254\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.004898\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.037988\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.009148\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.076054\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.018920\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.070982\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.011894\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.008444\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.016408\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.043583\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.002780\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.095862\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.001001\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.134840\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.016684\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.006071\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.021960\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.116316\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.025511\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.034113\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.002235\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.006152\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.004541\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.005337\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.076413\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.346585\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.050215\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.004787\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.070388\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.028807\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.028759\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.005603\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.004734\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.102801\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.015263\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.122992\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.002920\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.005390\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.021334\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.012906\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.002259\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.002151\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.025885\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.119295\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.002123\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.040089\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.001069\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.009677\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.072850\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.014001\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.201593\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.088978\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.030332\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.006020\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.003325\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.009375\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.013456\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.013519\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.071185\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.029326\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.000863\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.004118\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.014847\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.007247\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.018402\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.018910\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.038017\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.031172\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.019925\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.016360\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.002488\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.004691\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.010103\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.018822\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.002705\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.050961\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.010923\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.024974\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.009883\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.009749\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.004365\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.056412\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.005029\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.005035\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.100567\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.003260\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.002190\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.096033\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.053215\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001658\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.004992\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.018250\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.044204\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.076540\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.016652\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.004329\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.084570\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.000881\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.003012\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017331\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.190515\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.005024\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.010876\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.009596\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.013099\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.007800\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.019412\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.005630\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.007277\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.032126\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.139394\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.190461\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.003998\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.045514\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.003556\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.009832\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.000730\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.009472\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.032739\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.058090\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.021808\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.000679\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.017294\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.010823\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.078444\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.106020\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.004611\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.105774\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.015569\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.014777\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.013344\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.053371\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.019017\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.014151\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.006787\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.010643\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.244886\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.001284\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.049239\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.012787\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.059721\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.006671\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.033426\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.029658\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.007504\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.031626\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.017616\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.002014\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.007226\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.004942\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.005301\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.021057\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.105018\n",
            "\n",
            "Test set: Average loss: 0.0277, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.045857\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.038813\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.035630\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.017973\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.046720\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.004274\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.005252\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.002791\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.026983\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.002262\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.009723\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.013332\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.071401\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.000641\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.017538\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.006215\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.009547\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.006460\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.021684\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.002454\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.008755\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.003514\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.026485\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.002540\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.002272\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.001433\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.006867\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.006266\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.007554\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.002358\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.009516\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.006492\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.019972\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.074147\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001914\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.017940\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.050838\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.011553\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.000371\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.085915\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.004673\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.007135\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.001973\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.031980\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.000576\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.001073\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.071958\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.102356\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.016639\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.070995\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.016697\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.024473\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.065985\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.056942\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.002515\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.017841\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.005445\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.004716\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.009966\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.003677\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.046486\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.012433\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.017297\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.023652\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.001268\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.192182\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.013905\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.004612\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.003735\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.023158\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.019572\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.008972\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.004471\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.004687\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.040419\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.053955\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.004893\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.003907\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.000694\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.159797\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004912\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.006494\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.008296\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.002596\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.024468\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.020165\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.016749\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.047972\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.008299\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.030863\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.063759\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.093312\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.024462\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.004517\n",
            "\n",
            "Test set: Average loss: 0.0263, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.029426\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.004340\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.002224\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.012147\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.016155\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.001791\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.009760\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.007877\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.015174\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.016223\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.009376\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.062267\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.009504\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.029348\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.016623\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.008437\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.005012\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.068336\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.018532\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.001743\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.013511\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.002611\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.000899\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.067096\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.021734\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.063572\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.040574\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.010511\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.023540\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.022475\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.006458\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.017793\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.000658\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.020362\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.015175\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.004222\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.008908\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.045772\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.017326\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.023121\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.095277\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.001540\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.003216\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.000912\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.026519\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.020378\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.022871\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.028858\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.040344\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.001282\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.007614\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.155761\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.054262\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.002649\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.008886\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.063068\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.005446\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.016562\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.006760\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.003495\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.181026\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.024702\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.030163\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.001262\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001738\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.003147\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.003353\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.000970\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.002910\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.031750\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.012550\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.023186\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.004788\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.003796\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.049843\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.002921\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.016691\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001258\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.009432\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.017191\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.173737\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.022007\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.034687\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.075282\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.025973\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.004814\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.003741\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.010458\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.008498\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.056250\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.013440\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.004774\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.227453\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.001718\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9918/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.008976\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.010648\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.018680\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.011999\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.018624\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.062617\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.035460\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.009512\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.003421\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.001844\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.044231\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.041326\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.003594\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.175470\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.004704\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.003248\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.054425\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.076601\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.049820\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.035613\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.011972\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.058764\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.006305\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.007444\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.004360\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.000942\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.024404\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.032690\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.019649\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.003092\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.004238\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.017841\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.078394\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.058207\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.012388\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.183072\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.004326\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.008129\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.002720\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.025299\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.003751\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.005555\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.155886\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.007004\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.062383\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.019805\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.022739\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.018736\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.015395\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.013347\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.087776\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.006530\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.019018\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.040479\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.005882\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.013174\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.051113\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.001658\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.045887\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.099185\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.006845\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.030919\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.022430\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.004587\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.002989\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.097261\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.005662\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.015523\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.009325\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.047803\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.024431\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.097240\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.055132\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.025780\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.007703\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.026766\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.001503\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.000888\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.010838\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.113309\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.060489\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.038620\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.080054\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.001034\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.021227\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.003538\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.021559\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.004724\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.019628\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.073158\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.009972\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.010093\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.003899\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.067410\n",
            "\n",
            "Test set: Average loss: 0.0264, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000438\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.032231\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.000158\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.008426\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.007176\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.006838\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.163422\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.009438\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.001921\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.003616\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.062746\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.014794\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.001901\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.013013\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.009943\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.088653\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.016850\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.011793\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.083760\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.003977\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.013174\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.039830\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.038350\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.004172\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.028821\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.004417\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.069891\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.021579\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.039101\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.005253\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.015502\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.002394\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.048053\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.011959\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.102257\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.021530\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.008547\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.007809\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.006157\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.019251\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.008090\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.015552\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.019027\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.003928\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.035334\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.011455\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.001314\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.023997\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.030376\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.007545\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.005997\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.004391\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.007510\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.058450\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.121657\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.037800\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.001463\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.005102\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.003447\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.041773\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.003571\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.043874\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.044954\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.029484\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.019958\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.069761\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.042537\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.001054\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.104742\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.047921\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.015193\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.006510\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.072388\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.010861\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.005963\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.033244\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.084033\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.062402\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.010363\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.005602\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.078967\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.012975\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.020226\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.000675\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.003147\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.002248\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.093226\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.002637\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.004620\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.016639\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.041885\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.094535\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.001918\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.105438\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.010776\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.014387\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.004654\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.001114\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.005431\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.015775\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.146661\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.006453\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.026278\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.000384\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.008026\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.041208\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.012994\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.009436\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.035982\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.002148\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.033744\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.038448\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.002104\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.003993\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.003249\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.011367\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.023914\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.004796\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.003659\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.042887\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.024492\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.001037\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.006804\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.016209\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.072331\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.014700\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.006689\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.166852\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.001928\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.034911\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.001356\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.037436\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.007285\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.003286\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.020288\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.003823\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.013569\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.007816\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.167031\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.079442\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.009208\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.043639\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.001447\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.001914\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.000886\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.040233\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.006618\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.002142\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.004088\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.002587\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.022650\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.003065\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.037339\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.017441\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.016669\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.150677\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.000545\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.006690\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.087810\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.004506\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.026529\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.050911\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.012550\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.016492\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.005039\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.033996\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.013634\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.043244\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.108130\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.004214\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.047959\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.005228\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.007712\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.022783\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.022890\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.025859\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.002474\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.041341\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.089321\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.011835\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.003468\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.018799\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.080653\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.000682\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.006296\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.002693\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.013583\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.037661\n",
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9917/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.265192\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.002647\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.002304\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.011868\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.010332\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.009312\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.044490\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.000896\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.035934\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.006027\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.047520\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.023933\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.002860\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.008040\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.022970\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.019938\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.095019\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.033457\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.012702\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.002474\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.001274\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.005558\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.001723\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.000562\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.032907\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.015720\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.121849\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.023064\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.073995\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.123330\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.014034\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.019280\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.003744\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.000778\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.015874\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.008936\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.057742\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.002898\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.004338\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.004973\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.114368\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.029213\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.020417\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.011431\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.062330\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.031823\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.001169\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.085534\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.005340\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.021935\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.030184\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.135759\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.018774\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.019171\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.024220\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.016715\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.004129\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.002521\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.059678\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.165738\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.009111\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.038012\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.006274\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.006351\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.014599\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.020933\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.035399\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.016921\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.038955\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.007765\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.224702\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.083540\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.000822\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.044810\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.016514\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.011595\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.029266\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.010820\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.002432\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.007856\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.015733\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.067475\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.008070\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.000950\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.116084\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.004649\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.002151\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.052252\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.050510\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.001086\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.009900\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.005598\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.017147\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.004029\n",
            "\n",
            "Test set: Average loss: 0.0257, Accuracy: 9916/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM-TvDdYAdDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "811bce3e-a61c-4037-fe0f-81949e310b12"
      },
      "source": [
        "## load the model\n",
        "model = Net()\n",
        "state_dict = torch.load(\"mnist_cnn.pt\")\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATdyFBrbAc_E"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plh7hcmsB0xc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "786f57d8-4147-4b77-841c-4e4740dff60e"
      },
      "source": [
        "image, label = dataset2[100]\n",
        "image = image.unsqueeze(0)\n",
        "output = model(image)\n",
        "output =torch.argmax(output)\n",
        "print(output, label, output == label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6) 6 tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOf4AYJnHajs"
      },
      "source": [
        "Let's convert the Mnist trained model into onnx with the help of torch.onnx native support"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xXbdaM9B7Qz"
      },
      "source": [
        "torch.onnx.export(\n",
        "    model, ## pass model\n",
        "    (image), ## pass inpout example\n",
        "    \"mnist.onnx\", ##output path\n",
        "    input_names = ['input'], ## Pass names as per model input name\n",
        "    output_names = ['output'], ## Pass names as per model output name\n",
        "    opset_version=11, ##  export the model to the  opset version of the onnx submodule.\n",
        "    dynamic_axes = { ## this will makes export more generalize to take batch for prediction\n",
        "        'input' : {0: 'batch', 1: 'sequence'},\n",
        "        'output' : {0: 'batch'},        \n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NSgnaXgHxhk"
      },
      "source": [
        "For inference we will use Onnxruntime package which will give us boost as per our hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sZnbvWJCP0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9cbbc4ba-9abf-4202-a15b-1234af3e1263"
      },
      "source": [
        "!pip install onnxruntime\n",
        "# !pip install onnxruntime-gpu for gpu\n",
        "from onnxruntime import InferenceSession, SessionOptions, get_all_providers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (49.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ZhRqK5IK2A"
      },
      "source": [
        "For making prediction from our onnx model we have to create InferenceSession with the provider which is your hardware compatible execution provider as here we are using CPUExecutionProvider. You can improve it by giving more properties in options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_0OdD-wCUBm"
      },
      "source": [
        "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
        "  \n",
        "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
        "\n",
        "  # Few properties than might have an impact on performances (provided by MS)\n",
        "  options = SessionOptions()\n",
        "  options.intra_op_num_threads = 1\n",
        "\n",
        "  # Load the model as a graph and prepare the CPU backend \n",
        "  return InferenceSession(model_path, options, providers=[provider])\n",
        "cpu_model = create_model_for_provider(\"mnist.onnx\", \"CPUExecutionProvider\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCg51beLJDbt"
      },
      "source": [
        "Let's compare pytorch and onnx prediction time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQkx3bDYCbvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49ec2409-e5f7-4345-8c12-584559f431f0"
      },
      "source": [
        "%%time\n",
        "out = model(image) # Pytorch model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.5 ms, sys: 36 µs, total: 3.54 ms\n",
            "Wall time: 6.59 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTRiwLLmCbvO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5f06a231-6008-48f3-82ff-064c76fbf3ba"
      },
      "source": [
        "#onnx model\n",
        "%%time\n",
        "inputs_onnx= {'input':image.numpy()} ## same name as passes in onnx.export\n",
        "output = cpu_model.run(None, inputs_onnx) ## Here first arguments None becuase we want every output sometimes model return more than one output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 783 µs, sys: 982 µs, total: 1.76 ms\n",
            "Wall time: 1.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeuFs2yJJgxX"
      },
      "source": [
        "Make sure our model giving correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHGczNbHCbvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2c9700e-fabf-43a4-b05d-a62783ed1ec6"
      },
      "source": [
        "output =torch.argmax(torch.tensor(output))\n",
        "print(output, label, output == label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6) 6 tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN2SeIRl-kG4"
      },
      "source": [
        "## Alexnet pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-eiP6IkJtKd"
      },
      "source": [
        "In this section we will see how to convert pretrained models into onnx and play with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR7ic7QE4SL1"
      },
      "source": [
        "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHsR7Vza5HqE"
      },
      "source": [
        "!wget https://www.learnopencv.com/wp-content/uploads/2019/05/dog.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eskgZ1b34eCQ"
      },
      "source": [
        "import json\n",
        "idx2label = []\n",
        "cls2label = {}\n",
        "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
        "    class_idx = json.load(read_file)\n",
        "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtUn-B2-2FjT"
      },
      "source": [
        "!pip install torchvision\n",
        "from torchvision import models\n",
        "import torch\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([            #[1]\n",
        " transforms.Resize(256),                    #[2]\n",
        " transforms.CenterCrop(224),                #[3]\n",
        " transforms.ToTensor(),                     #[4]\n",
        " transforms.Normalize(                      #[5]\n",
        " mean=[0.485, 0.456, 0.406],                #[6]\n",
        " std=[0.229, 0.224, 0.225]                  #[7]\n",
        " )])\n",
        "from PIL import Image\n",
        "img = Image.open(\"dog.jpg\")\n",
        "img_t = transform(img)\n",
        "batch_t = torch.unsqueeze(img_t, 0)\n",
        "alexnet.eval()\n",
        "out = alexnet(batch_t)\n",
        "\n",
        "_, index = torch.max(out, 1)\n",
        "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHK-B8fE3ZKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0eca11b-81a8-4dba-f364-838467349ca7"
      },
      "source": [
        "print(idx2label[index[0]], percentage[index[0]].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labrador_retriever 41.58518600463867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZa7zHq35pZ-"
      },
      "source": [
        "torch.onnx.export(\n",
        "    alexnet, ## pass model\n",
        "    (batch_t), ## pass inpout example\n",
        "    \"alexnet.onnx\", ##output path\n",
        "    input_names = ['input'], ## Pass names as per model input name\n",
        "    output_names = ['output'], ## Pass names as per model output name\n",
        "    opset_version=11, ##  export the model to the  opset version of the onnx submodule.\n",
        "    dynamic_axes = { ## this will makes export more generalize to take batch for prediction\n",
        "        'input' : {0: 'batch', 1: 'sequence'},\n",
        "        'output' : {0: 'batch'},        \n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbSNIXkw8kPE"
      },
      "source": [
        "!pip install onnxruntime\n",
        "from onnxruntime import InferenceSession, SessionOptions, get_all_providers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvmQNjDa8wts"
      },
      "source": [
        "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
        "  \n",
        "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
        "\n",
        "  # Few properties than might have an impact on performances (provided by MS)\n",
        "  options = SessionOptions()\n",
        "  options.intra_op_num_threads = 1\n",
        "\n",
        "  # Load the model as a graph and prepare the CPU backend \n",
        "  return InferenceSession(model_path, options, providers=[provider])\n",
        "cpu_model = create_model_for_provider(\"alexnet.onnx\", \"CPUExecutionProvider\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbo844Z7-bMf"
      },
      "source": [
        "Pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXKfw5lt-VP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e837040c-955c-4327-ad67-dd82eeb6dd6f"
      },
      "source": [
        "%%time\n",
        "out = alexnet(batch_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 56.1 ms, sys: 1.72 ms, total: 57.8 ms\n",
            "Wall time: 61.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUBT3rfK-ded"
      },
      "source": [
        "onnx model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJP37-6086vB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39b1a95d-0516-401a-9b0b-f358ceb6b597"
      },
      "source": [
        "%%time\n",
        "inputs_onnx= {'input':batch_t.numpy()} ## same name as passes in onnx.export\n",
        "output = cpu_model.run(None, inputs_onnx) ## Here first arguments None becuase we want every output sometimes model return more than one output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 42.9 ms, sys: 60 µs, total: 42.9 ms\n",
            "Wall time: 43.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYwMNFO29eK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "320a5ddc-b3c8-4a28-fb5c-68517d18d5e8"
      },
      "source": [
        "output = torch.tensor(output[0])\n",
        "_, index = torch.max(output, 1)\n",
        "percentage = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
        "print(idx2label[index[0]], percentage[index[0]].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labrador_retriever 41.58515167236328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnOnpAO9jxe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsikc2hJC-Al"
      },
      "source": [
        "## Bert HuggingFace QAModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ddQAZFyDh6c"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ30a-Z_DCB9"
      },
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aauu10aYDftZ"
      },
      "source": [
        "question = \"what is google specialization\"\n",
        "text = \"Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware.\"\n",
        "encoding = tokenizer.encode_plus(question, text)\n",
        "input_ids, attention_mask, token_type_ids = encoding[\"input_ids\"],encoding[\"attention_mask\"], encoding[\"token_type_ids\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3aKQhanEC1u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98be7ce0-05b1-4eb4-aee4-ad0c08816312"
      },
      "source": [
        "%%time \n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 564 ms, sys: 26.8 ms, total: 591 ms\n",
            "Wall time: 689 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQdxIv6CFDZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b67c1576-5e72-4e22-cab8-28c3868dd503"
      },
      "source": [
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "an american multinational technology company that specializes in internet - related services and products\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHLKtKGtFvTm"
      },
      "source": [
        "### onnx method 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLPQzllsFzy7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8dc9b41f-72f9-4d14-8c22-906e84ed4c96"
      },
      "source": [
        "input_ids = torch.tensor([input_ids])\n",
        "attention_mask = torch.tensor([attention_mask])\n",
        "token_type_ids = torch.tensor([token_type_ids])\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (input_ids,attention_mask, token_type_ids),\n",
        "    \"qa.onnx\",\n",
        "    input_names = ['input_ids','attention_mask', 'token_type_ids'], ## Be carefule to write this names\n",
        "    output_names = ['qa_outputs'], ## Be carefule to write this names\n",
        "    opset_version=11,\n",
        "    dynamic_axes = {\n",
        "        'input_ids' : {0: 'batch', 1: 'sequence'},\n",
        "        'attention_mask' : {0: 'batch', 1: 'sequence'}, \n",
        "        'token_type_ids' : {0: 'batch', 1: 'sequence'}, \n",
        "        'qa_outputs': {0: 'batch'}\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py:197: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  position_ids = self.position_ids[:, :seq_length]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU0cdMDiIqwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a2ed8b56-b082-4e95-b84d-c01dcca52ba6"
      },
      "source": [
        "!pip install onnxruntime\n",
        "from onnxruntime import InferenceSession, SessionOptions, get_all_providers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (49.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGoagiqFIlXZ"
      },
      "source": [
        "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
        "  \n",
        "  assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
        "\n",
        "  # Few properties than might have an impact on performances (provided by MS)\n",
        "  options = SessionOptions()\n",
        "  options.intra_op_num_threads = 1\n",
        "\n",
        "  # Load the model as a graph and prepare the CPU backend \n",
        "  return InferenceSession(model_path, options, providers=[provider])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYbnviEVFzxw"
      },
      "source": [
        "cpu_model = create_model_for_provider(\"qa.onnx\", \"CPUExecutionProvider\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaPf6g7yGt-G"
      },
      "source": [
        "inputs_onnx= {\n",
        "    'input_ids' : input_ids.numpy(),\n",
        "    'attention_mask' : attention_mask.numpy(), \n",
        "    'token_type_ids' : token_type_ids.numpy(),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYbPJNXFGn7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f2e2515-4bca-491e-81dc-8653c68c8a6f"
      },
      "source": [
        "%%time\n",
        "start_scores, end_scores = cpu_model.run(None, inputs_onnx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 629 ms, sys: 1.2 ms, total: 631 ms\n",
            "Wall time: 632 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRw7Z2oeIzG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "609ce748-19ff-431e-98b5-b3371c32407e"
      },
      "source": [
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(torch.tensor(start_scores)) : torch.argmax(torch.tensor(end_scores))+1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "an american multinational technology company that specializes in internet - related services and products\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWMpSPHzGhoF"
      },
      "source": [
        "### onnx method for hugging face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORebFIYHbmu"
      },
      "source": [
        "tokenizer.save_pretrained(\"qa2/\")\n",
        "model.save_pretrained(\"qa2/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJsWEsXWFg_y"
      },
      "source": [
        "from pathlib import Path\n",
        "path = Path(\"onnx/qa2.onnx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnKyb6h6FUQx"
      },
      "source": [
        "!rm -rf onnx/\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "\n",
        "# Handles all the above steps for you\n",
        "convert(framework=\"pt\",  ## pt for pytorch\n",
        "        model=\"qa2\",     ## model path\n",
        "        output=path, \n",
        "        opset=11,\n",
        "        pipeline_name = \"question-answering\") ## pipeline_name is most important when you use this function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXsg4roXHuSd"
      },
      "source": [
        "cpu_model = create_model_for_provider(\"onnx/qa2.onnx\", \"CPUExecutionProvider\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by99NQKTJ2JN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b802d00-ae9f-4bef-c630-8026ba75659a"
      },
      "source": [
        "%%time\n",
        "start_scores, end_scores = cpu_model.run(None, inputs_onnx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 655 ms, sys: 857 µs, total: 655 ms\n",
            "Wall time: 658 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAHt5WFAKAgc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25cafdef-5c05-4366-f041-e139d3387124"
      },
      "source": [
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(torch.tensor(start_scores)) : torch.argmax(torch.tensor(end_scores))+1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "an american multinational technology company that specializes in internet - related services and products\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF12qYuJKEZp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Re06zSLiDz"
      },
      "source": [
        "**Usefull links**\n",
        "\n",
        "https://github.com/onnx/onnx\n",
        "\n",
        "https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333\n",
        "\n",
        "https://blog.paperspace.com/what-every-ml-ai-developer-should-know-about-onnx/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_0W5wqGmtM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}